{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Goals:\n- Assign labels to text\n- Giving meaning to words and sentences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# An Additional Encoding Technique - Word Embedding ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nwords = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\nword_to_idx = {word: i for i, word in enumerate(words)}\ninputs = torch.LongTensor([word_to_idx[w] for w in words])\nembedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\noutput = embedding(inputs)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T22:59:33.711564Z","iopub.execute_input":"2024-01-15T22:59:33.711932Z","iopub.status.idle":"2024-01-15T22:59:33.810227Z","shell.execute_reply.started":"2024-01-15T22:59:33.711905Z","shell.execute_reply":"2024-01-15T22:59:33.809321Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([[-0.9045, -0.2070,  0.3700,  0.3183,  0.3293,  0.6960,  1.7425,  0.4441,\n         -2.4203, -1.2003],\n        [ 0.1203, -0.4481,  0.2451, -1.5981,  0.1387,  0.3480,  0.3555, -0.0669,\n          2.1116, -0.2849],\n        [ 0.1232, -1.9137,  1.4338,  0.5739, -0.5699,  0.3137, -1.2908,  0.3347,\n         -2.7589, -0.8176],\n        [-0.6698,  0.3998, -2.2533, -1.3561, -0.4504, -1.0135, -0.5426, -0.5256,\n         -0.5288, -0.2817],\n        [-1.7797,  0.7000, -1.2241, -0.5565,  0.9344,  1.4716,  0.3707, -0.8803,\n          0.3429,  0.9646],\n        [ 0.8172, -0.0602, -0.5270,  0.7679, -1.8846, -0.7690, -0.4009,  0.0184,\n          0.1815, -0.9195]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Binary Classification\ne.g. spam vs non-spam email","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-class Classification\n\ne.g. snow vs rainy vs sunny vs cloudy vs ...","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:02:38.361396Z","iopub.execute_input":"2024-01-14T18:02:38.362378Z","iopub.status.idle":"2024-01-14T18:02:38.369772Z","shell.execute_reply.started":"2024-01-14T18:02:38.362337Z","shell.execute_reply":"2024-01-14T18:02:38.368358Z"}}},{"cell_type":"markdown","source":"# Multi-label Classification\n\ne.g. a book could have multiple labels assigned to it like fantasy, action, adventure, etc","metadata":{"execution":{"iopub.status.busy":"2024-01-14T18:03:14.356109Z","iopub.execute_input":"2024-01-14T18:03:14.356503Z","iopub.status.idle":"2024-01-14T18:03:14.364546Z","shell.execute_reply.started":"2024-01-14T18:03:14.356471Z","shell.execute_reply":"2024-01-14T18:03:14.362772Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN for Text Classification\n\nArchitecture:\n1. Convolutional layer: applies filters to input data\n2. Pooling layer: reduces data size while preserving important information\n3. Fully connected layer: makes final predictions based on previous layer output","metadata":{}},{"cell_type":"markdown","source":"## Sentiment Analysis using CNN","metadata":{}},{"cell_type":"code","source":"class SentimentAnalysisCNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # we add a 1d convoluation layer\n        # it is preferred over 2d as text data is 1d\n        self.conv = nn.Conv1d(embed_dim, \n                              embed_dim,\n                             kernel_size=3, \n                             stride=1,\n                             padding=1\n                             )\n        self.fc = nn.Linear(embed_dim, 2)\n        \n    def forward(self, text):\n        # embedding layer converts text to embedding\n        embeded = self.embedding(text).permute(0, 2, 1)\n        # the permute at the end match tensors to convolution layer's\n        # expected input\n        \n        # extract important features with ReLu\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim = 2)\n        \n        return self.fc(conved)\n        \n        \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A simple example\nfrom nltk.tokenize import word_tokenize\nvocab = \"I love this book! I do not like this book!\"\ntokens = word_tokenize(vocab)\nprint(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\n\ntokenizer = get_tokenizer(\"basic_english\")\nprint(tokenizer.fit_transform(vocab))","metadata":{"execution":{"iopub.status.busy":"2024-01-15T22:54:38.959791Z","iopub.execute_input":"2024-01-15T22:54:38.960118Z","iopub.status.idle":"2024-01-15T22:54:39.107963Z","shell.execute_reply.started":"2024-01-15T22:54:38.960093Z","shell.execute_reply":"2024-01-15T22:54:39.106746Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m(vocab))\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'fit_transform'"],"ename":"AttributeError","evalue":"'function' object has no attribute 'fit_transform'","output_type":"error"}]},{"cell_type":"code","source":"\nword_to_idx = {word: i for i, word in enumerate(vocab)}\nvocab_size = len(word_to_idx)\nembed_dim = 10\n\nbook_samples = [\n    (\"The story was captivating and kept me hooked until the end\".split(),1),\n    (\"I found the characters shallow and the plot predictable\".split(),0)\n]\n\nmodel = SentimentAnalysisCNN(vocab_size, embed_dim)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T22:59:19.238428Z","iopub.execute_input":"2024-01-15T22:59:19.238746Z","iopub.status.idle":"2024-01-15T22:59:19.269263Z","shell.execute_reply.started":"2024-01-15T22:59:19.238722Z","shell.execute_reply":"2024-01-15T22:59:19.267422Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      5\u001b[0m book_samples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe story was captivating and kept me hooked until the end\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(),\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI found the characters shallow and the plot predictable\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(),\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentAnalysisCNN\u001b[49m(vocab_size, embed_dim)\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'SentimentAnalysisCNN' is not defined"],"ename":"NameError","evalue":"name 'SentimentAnalysisCNN' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}